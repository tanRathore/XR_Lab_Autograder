{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Converting vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.docx â†’ vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf (56 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from emralinolalaine_LATE_162831_14938886_Glenn Murcutt_ â€œTouching the Earth Lightlyâ€.pdf (14 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from krukjulia_LATE_198551_15046680_Eero Saarinen_ Cogs 160 Research Document.pdf (49 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from spavenchristine_LATE_96300_14929508_COGS 160_1 (1).pdf (42 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from khirwadkarisha_166304_14925482_Report.pdf (36 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from liangmichael_188529_14924464_COGS 160 - Docs.pdf (58 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from yangheiman_LATE_190478_14963531_COGS160.pdf (43 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from wucynthia_LATE_167097_15019933_COGS 160.pdf (1 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from khirwadkarisha_166304_14925456_ken yeang cogs 160 in class.pdf (34 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from hsucalvin_166834_14967682_Cogs 160 Slide and Doc Links for Turn-in.pdf (1 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from dasilvatheo_LATE_171930_14930244_HW A1.pdf (46 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf (43 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from marvanalicia_212624_14925657_A.Marvan_LuisBarragan.pdf (3 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from delacruzrenier_LATE_226065_14930691_Paul Rudolph_ Life, Work, and Enduring Influence.pdf (6 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf (61 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Extracting images from liangmichael_188529_14924465_Michael - COGS 160.pdf (103 pages)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Extracted 913 images; metadata saved to Extracted_images/image_metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz       # PyMuPDF\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from docx2pdf import convert\n",
    "\n",
    "def preprocess_docs(input_dir: str):\n",
    "    \"\"\"\n",
    "    Convert all .docx/.doc files in input_dir into PDFs\n",
    "    with the same base filename.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(input_dir):\n",
    "        lower = filename.lower()\n",
    "        if lower.endswith((\".docx\", \".doc\")):\n",
    "            doc_path = os.path.join(input_dir, filename)\n",
    "            pdf_path = os.path.join(\n",
    "                input_dir,\n",
    "                os.path.splitext(filename)[0] + \".pdf\"\n",
    "            )\n",
    "            try:\n",
    "                print(f\"ğŸ”§ Converting {filename} â†’ {os.path.basename(pdf_path)} â€¦\")\n",
    "                convert(doc_path, pdf_path)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to convert {filename}: {e}\")\n",
    "\n",
    "def extract_images_from_pdfs(\n",
    "    input_dir: str,\n",
    "    output_dir: str,\n",
    "    min_width: int = 1200,\n",
    "    metadata_filename: str = \"image_metadata.csv\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1. Convert any .doc/.docx to .pdf.\n",
    "    2. Walk through each PDF in input_dir, extract all embedded images,\n",
    "    3. Save images under output_dir/<pdf_basename>/â€¦,\n",
    "    4. Compile a CSV of metadata (file, page, dimensions, resolution, high-res flag).\n",
    "    Returns the metadata DataFrame.\n",
    "    \"\"\"\n",
    "    # Step 1: convert Word docs\n",
    "    preprocess_docs(input_dir)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    records = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        pdf_path    = os.path.join(input_dir, filename)\n",
    "        base_name   = os.path.splitext(filename)[0]\n",
    "        doc         = fitz.open(pdf_path)\n",
    "\n",
    "        # Create subfolder for this PDFâ€™s images\n",
    "        pdf_img_dir = os.path.join(output_dir, base_name)\n",
    "        os.makedirs(pdf_img_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"\\nğŸ“„ Extracting images from {filename} ({len(doc)} pages)â€¦\")\n",
    "        for page_idx in tqdm(range(len(doc)), desc=\"Pages\", leave=False):\n",
    "            page   = doc[page_idx]\n",
    "            images = page.get_images(full=True)\n",
    "            if not images:\n",
    "                continue\n",
    "\n",
    "            for img_idx, img_info in enumerate(images, start=1):\n",
    "                xref      = img_info[0]\n",
    "                img_data  = doc.extract_image(xref)\n",
    "                img_bytes = img_data[\"image\"]\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(img_bytes))\n",
    "                except Exception as e:\n",
    "                    print(f\" âš ï¸ Couldnâ€™t open image on page {page_idx+1}, idx {img_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                w, h     = img.size\n",
    "                img_name = f\"{base_name}_p{page_idx+1}_img{img_idx}.png\"\n",
    "                img_path = os.path.join(pdf_img_dir, img_name)\n",
    "                img.save(img_path)\n",
    "\n",
    "                records.append({\n",
    "                    \"pdf_file\":    filename,\n",
    "                    \"page\":        page_idx + 1,\n",
    "                    \"image_name\":  img_name,\n",
    "                    \"image_path\":  img_path,\n",
    "                    \"width\":       w,\n",
    "                    \"height\":      h,\n",
    "                    \"resolution\":  f\"{w}x{h}\",\n",
    "                    \"is_high_res\": w >= min_width\n",
    "                })\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "    # Write out metadata CSV\n",
    "    if records:\n",
    "        df       = pd.DataFrame(records)\n",
    "        csv_path = os.path.join(output_dir, metadata_filename)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nâœ… Extracted {len(records)} images; metadata saved to {csv_path}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"âš ï¸ No images found in any PDF.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# â”€â”€ Example usage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    metadata_df = extract_images_from_pdfs(\n",
    "        input_dir=\"submissions\",\n",
    "        output_dir=\"Extracted_images\",\n",
    "        min_width=1200\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2pdf\n",
      "  Using cached docx2pdf-0.1.8-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting appscript>=1.1.0 (from docx2pdf)\n",
      "  Downloading appscript-1.3.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (417 bytes)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in ./venv311/lib/python3.11/site-packages (from docx2pdf) (4.67.1)\n",
      "Collecting lxml>=4.7.1 (from appscript>=1.1.0->docx2pdf)\n",
      "  Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Using cached docx2pdf-0.1.8-py3-none-any.whl (6.7 kB)\n",
      "Downloading appscript-1.3.0-cp311-cp311-macosx_10_9_universal2.whl (99 kB)\n",
      "Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, appscript, docx2pdf\n",
      "Successfully installed appscript-1.3.0 docx2pdf-0.1.8 lxml-5.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install docx2pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Compressing mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf\n",
      "ğŸ”„ Compressing emralinolalaine_LATE_162831_14938886_Glenn Murcutt_ â€œTouching the Earth Lightlyâ€.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/emralinolalaine_LATE_162831_14938886_Glenn Murcutt_ â€œTouching the Earth Lightlyâ€.pdf\n",
      "ğŸ”„ Compressing krukjulia_LATE_198551_15046680_Eero Saarinen_ Cogs 160 Research Document.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/krukjulia_LATE_198551_15046680_Eero Saarinen_ Cogs 160 Research Document.pdf\n",
      "ğŸ”„ Compressing spavenchristine_LATE_96300_14929508_COGS 160_1 (1).pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/spavenchristine_LATE_96300_14929508_COGS 160_1 (1).pdf\n",
      "ğŸ”„ Compressing khirwadkarisha_166304_14925482_Report.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/khirwadkarisha_166304_14925482_Report.pdf\n",
      "ğŸ”„ Compressing liangmichael_188529_14924464_COGS 160 - Docs.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/liangmichael_188529_14924464_COGS 160 - Docs.pdf\n",
      "ğŸ”„ Compressing yangheiman_LATE_190478_14963531_COGS160.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/yangheiman_LATE_190478_14963531_COGS160.pdf\n",
      "ğŸ”„ Compressing wucynthia_LATE_167097_15019933_COGS 160.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/wucynthia_LATE_167097_15019933_COGS 160.pdf\n",
      "ğŸ”„ Compressing khirwadkarisha_166304_14925456_ken yeang cogs 160 in class.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/khirwadkarisha_166304_14925456_ken yeang cogs 160 in class.pdf\n",
      "ğŸ”„ Compressing hsucalvin_166834_14967682_Cogs 160 Slide and Doc Links for Turn-in.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/hsucalvin_166834_14967682_Cogs 160 Slide and Doc Links for Turn-in.pdf\n",
      "ğŸ”„ Compressing dasilvatheo_LATE_171930_14930244_HW A1.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/dasilvatheo_LATE_171930_14930244_HW A1.pdf\n",
      "ğŸ”„ Compressing davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\n",
      "ğŸ”„ Compressing marvanalicia_212624_14925657_A.Marvan_LuisBarragan.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/marvanalicia_212624_14925657_A.Marvan_LuisBarragan.pdf\n",
      "ğŸ”„ Compressing delacruzrenier_LATE_226065_14930691_Paul Rudolph_ Life, Work, and Enduring Influence.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/delacruzrenier_LATE_226065_14930691_Paul Rudolph_ Life, Work, and Enduring Influence.pdf\n",
      "ğŸ”„ Compressing vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf\n",
      "ğŸ”„ Compressing liangmichael_188529_14924465_Michael - COGS 160.pdf ...\n",
      "âœ… Saved compressed PDF: compressed_files/liangmichael_188529_14924465_Michael - COGS 160.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz      # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def compress_all_pdfs(input_dir, output_dir, dpi=100, downscale_factor=2):\n",
    "    \"\"\"\n",
    "    Compress all PDF files in `input_dir` by rendering each page to an image,\n",
    "    optionally downscaling, and reassembling into a new PDF in `output_dir`.\n",
    "\n",
    "    :param input_dir: Folder containing original PDF files.\n",
    "    :param output_dir: Folder to write compressed PDFs.\n",
    "    :param dpi: DPI for rendering pages.\n",
    "    :param downscale_factor: Factor by which to downscale rendered images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        print(f\"ğŸ”„ Compressing {filename} ...\")\n",
    "        try:\n",
    "            doc = fitz.open(input_path)\n",
    "            new_pdf = fitz.open()\n",
    "\n",
    "            for page in doc:\n",
    "                pix = page.get_pixmap(dpi=dpi)\n",
    "                # create PIL image from raw samples\n",
    "                img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
    "\n",
    "                # downscale using LANCZOS filter\n",
    "                new_size = (pix.width // downscale_factor, pix.height // downscale_factor)\n",
    "                img = img.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "                # save into a one-page PDF in memory\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format=\"PDF\", resolution=dpi)\n",
    "                buffer.seek(0)\n",
    "\n",
    "                img_pdf = fitz.open(\"pdf\", buffer)\n",
    "                new_pdf.insert_pdf(img_pdf)\n",
    "\n",
    "            new_pdf.save(output_path)\n",
    "            new_pdf.close()\n",
    "            doc.close()\n",
    "            print(f\"âœ… Saved compressed PDF: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to compress {filename}: {e}\")\n",
    "\n",
    "\n",
    "INPUT_DIR  = \"submissions\"\n",
    "OUTPUT_DIR = \"compressed_files\"\n",
    "\n",
    "compress_all_pdfs(\n",
    "    input_dir=INPUT_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    dpi=100,\n",
    "    downscale_factor=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Unmatched login_ids: ['mainayardaniel' 'emralinolalaine' 'krukjulia' 'spavenchristine'\n",
      " 'khirwadkarisha' 'liangmichael' 'yangheiman' 'dasilvatheo' 'davidmatthew'\n",
      " 'marvanalicia' 'vidyalatanvi']\n",
      "\n",
      "âœ… Done! Check â†’ image_metadata_with_name_pid.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# â”€â”€ 1) Load CSVs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "meta_df   = pd.read_csv(\"image_metadata.csv\")\n",
    "roster_df = pd.read_csv(\"student_info.csv\")\n",
    "\n",
    "# â”€â”€ 2) Extract login_id from your PDF filenames â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#    e.g. \"mainayardaniel_127050_14924649_COGS160â€¦\" â†’ \"mainayardaniel\"\n",
    "meta_df[\"login_id\"] = meta_df[\"pdf_file\"].str.split(\"_\").str[0]\n",
    "\n",
    "# â”€â”€ 3) Tidy roster columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "roster_df = roster_df.rename(columns={\n",
    "    \"SIS Login ID\": \"login_id\",\n",
    "    \"Student\":      \"student_name\",\n",
    "    \"SIS User ID\":  \"pid\"\n",
    "})\n",
    "# ensure no NaNs and strip whitespace\n",
    "roster_df[\"student_name\"] = roster_df[\"student_name\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# â”€â”€ 4) Exactâ€match merge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "merged = pd.merge(\n",
    "    meta_df,\n",
    "    roster_df[[\"login_id\",\"student_name\",\"pid\"]],\n",
    "    on=\"login_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# â”€â”€ 5) Which login_ids still have no PID? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "unmatched = merged.loc[merged[\"pid\"].isna(), \"login_id\"].unique()\n",
    "print(\"ğŸ” Unmatched login_ids:\", unmatched)\n",
    "\n",
    "# â”€â”€ 6) Prepare for fuzzy matching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#    create a normalized key (lowercase, no punctuation/space)\n",
    "roster_df[\"norm_name\"] = (\n",
    "    roster_df[\"student_name\"]\n",
    "      .str.lower()\n",
    "      .str.replace(r\"[^a-z0-9]\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# map norm_name â†’ (login_id, student_name, pid)\n",
    "roster_map = {\n",
    "    row.norm_name: (row.login_id, row.student_name, row.pid)\n",
    "    for row in roster_df.itertuples()\n",
    "}\n",
    "\n",
    "# â”€â”€ 7) Build fuzzyâ€match suggestions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "suggestions = {}\n",
    "for uid in unmatched:\n",
    "    key = str(uid).lower()\n",
    "    # 7a) exact normalized-name\n",
    "    if key in roster_map:\n",
    "        suggestions[uid] = [roster_map[key]]\n",
    "        continue\n",
    "    # 7b) substring match\n",
    "    hits = [roster_map[n] for n in roster_map if key in n or n in key]\n",
    "    if hits:\n",
    "        suggestions[uid] = hits\n",
    "        continue\n",
    "    # 7c) difflib fallback\n",
    "    best = get_close_matches(key, roster_map.keys(), n=1, cutoff=0.6)\n",
    "    suggestions[uid] = [roster_map[best[0]]] if best else []\n",
    "\n",
    "# â”€â”€ 8) Auto-fill those with exactly one candidate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for uid, matches in suggestions.items():\n",
    "    if len(matches) == 1:\n",
    "        _, name, pid = matches[0]\n",
    "        merged.loc[merged[\"login_id\"] == uid, \"student_name\"] = name\n",
    "        merged.loc[merged[\"login_id\"] == uid, \"pid\"]          = pid\n",
    "\n",
    "# â”€â”€ 9) Print any truly ambiguous or missing cases â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for uid, matches in suggestions.items():\n",
    "    if len(matches) > 1:\n",
    "        print(f\"\\nâš ï¸ {uid!r} HAS MULTIPLE MATCHES:\")\n",
    "        for _, name, pid in matches:\n",
    "            print(f\"    {name} â†’ {pid}\")\n",
    "    elif not matches:\n",
    "        print(f\"\\nâŒ {uid!r} HAS NO CLOSE MATCH\")\n",
    "\n",
    "# â”€â”€10) Save the finished CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "out_path = \"image_metadata_with_name_pid.csv\"\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(f\"\\nâœ… Done! Check â†’ {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XR_Lab (py3.11)",
   "language": "python",
   "name": "xr_lab_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
